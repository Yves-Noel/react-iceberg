{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit"
  },
  "interpreter": {
   "hash": "64eda458c7f1b075005dedfab76b92ce33553dadf338fff8a20392edb47496f5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import requests\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\") \\\n",
    "    .appName(\"react-iceberg\") \\\n",
    "    .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\",\"hive\") \\\n",
    "    .config(\"spark.sql.catalog.local\",\"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\",\"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\",\"/workspaces/react-iceberg/data/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.sql(\"select 'spark' as hello \")\n",
    "\n",
    "df.show()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+\n|hello|\n+-----+\n|spark|\n+-----+\n\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/workspaces/react-iceberg/proxylogon_ssrf_rce_poc_2021-03-14T01401970.json'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "url = 'https://raw.githubusercontent.com/OTRF/mordor/master/datasets/small/windows/persistence/host/proxylogon_ssrf_rce_poc.zip'\n",
    "zipFileRequest = requests.get(url)\n",
    "zipFile = ZipFile(BytesIO(zipFileRequest.content))\n",
    "jsonFilePath = zipFile.extract(zipFile.namelist()[0],\"./data\")\n",
    "jsonFilePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------------+---------------------------------------------------+-------+-----+\n|Hostname              |Channel                                            |EventID|count|\n+----------------------+---------------------------------------------------+-------+-----+\n|MXS01.azsentinel.local|Microsoft-Windows-Sysmon/Operational               |10     |3626 |\n|MXS01.azsentinel.local|Security                                           |4658   |2398 |\n|MXS01.azsentinel.local|Security                                           |4656   |1199 |\n|MXS01.azsentinel.local|Security                                           |4690   |1199 |\n|MXS01.azsentinel.local|Microsoft-Windows-Sysmon/Operational               |7      |427  |\n|MXS01.azsentinel.local|Microsoft-Windows-Sysmon/Operational               |11     |301  |\n|MXS01.azsentinel.local|Microsoft-Exchange-ActiveMonitoring/MonitorResult  |3      |293  |\n|MXS01.azsentinel.local|Microsoft-Exchange-ActiveMonitoring/ResponderResult|4      |167  |\n|MXS01.azsentinel.local|Security                                           |5156   |132  |\n|MXS01.azsentinel.local|Microsoft-Windows-Sysmon/Operational               |12     |120  |\n|MXS01.azsentinel.local|Microsoft-Windows-Sysmon/Operational               |3      |117  |\n|MXS01.azsentinel.local|Security                                           |5158   |85   |\n|MXS01.azsentinel.local|Security                                           |4703   |72   |\n|MXS01.azsentinel.local|Microsoft-Exchange-ActiveMonitoring/ProbeResult    |2      |57   |\n|MXS01.azsentinel.local|Microsoft-Windows-Sysmon/Operational               |13     |37   |\n|MXS01.azsentinel.local|Security                                           |4627   |21   |\n|MXS01.azsentinel.local|Security                                           |4624   |21   |\n|MXS01.azsentinel.local|Security                                           |4985   |18   |\n|MXS01.azsentinel.local|Application                                        |2303   |17   |\n|MXS01.azsentinel.local|Security                                           |4672   |16   |\n+----------------------+---------------------------------------------------+-------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Creating a Spark Dataframe\n",
    "df2 = spark.read.json(jsonFilePath)\n",
    "# Validating Type of Output\n",
    "df2.createOrReplaceTempView(\"mordorTable\")\n",
    "\n",
    "spark.sql(\n",
    "'''\n",
    "SELECT Hostname,Channel,EventID, Count(*) as count\n",
    "FROM mordorTable\n",
    "GROUP BY Hostname,Channel,EventID\n",
    "ORDER BY count DESC\n",
    "'''\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.format(\"iceberg\").saveAsTable(\"local.db.table\")               \n",
    "# THIS IS THE CORRECT SYNTAX FOR Spark 3.1+ (but Iceberg doesn't work with it yet)\n",
    "#     .writeTo(\"local.db.table\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}